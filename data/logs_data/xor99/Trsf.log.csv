Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 68, in predict_lstm
    model = getattr(models, model_name)(bs, timesteps, alphabet_size)
  File "/src/workspace/src/models.py", line 58, in Trsf
    model.add(TransformerBlock(embed_dim=32, num_heads=2, ff_dim=32))
  File "/src/workspace/src/models.py", line 25, in __init__
    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
AttributeError: module 'keras.layers' has no attribute 'MultiHeadAttention'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.39
	System time (seconds): 0.94
	Percent of CPU this job got: 121%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 878792
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 154423
	Voluntary context switches: 38
	Involuntary context switches: 12494
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.38
	System time (seconds): 0.70
	Percent of CPU this job got: 133%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.30
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 235452
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48959
	Voluntary context switches: 46
	Involuntary context switches: 28
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 22, in <module>
    class MultiHeadAttention(Layer):
NameError: name 'Layer' is not defined
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.44
	System time (seconds): 0.71
	Percent of CPU this job got: 132%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.37
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 234208
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48623
	Voluntary context switches: 33
	Involuntary context switches: 34
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 22, in <module>
    class MultiHeadAttention(Layer):
NameError: name 'Layer' is not defined
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.38
	System time (seconds): 0.57
	Percent of CPU this job got: 127%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233004
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48475
	Voluntary context switches: 35
	Involuntary context switches: 100199
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 68, in predict_lstm
    model = getattr(models, model_name)(bs, timesteps, alphabet_size)
  File "/src/workspace/src/models.py", line 656, in Trsf
    model.add(TransformerBlock(embed_dim=32, num_heads=2, ff_dim=32))
  File "/src/workspace/src/models.py", line 623, in __init__
    self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
  File "/src/workspace/src/models.py", line 149, in __init__
    self._kernel_initializer = initializers.get(kernel_initializer)
NameError: name 'initializers' is not defined
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.28
	System time (seconds): 0.96
	Percent of CPU this job got: 118%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 878676
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 156410
	Voluntary context switches: 47
	Involuntary context switches: 78882
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.46
	System time (seconds): 0.61
	Percent of CPU this job got: 132%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.33
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 235928
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48961
	Voluntary context switches: 43
	Involuntary context switches: 15516
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 23, in <module>
    import tensorflow.compat.v2 as tf
ModuleNotFoundError: No module named 'tensorflow.compat'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.42
	System time (seconds): 0.66
	Percent of CPU this job got: 133%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.30
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233408
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48576
	Voluntary context switches: 35
	Involuntary context switches: 31
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 23, in <module>
    import tensorflow.compat.v2 as tf
ModuleNotFoundError: No module named 'tensorflow.compat'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.44
	System time (seconds): 0.64
	Percent of CPU this job got: 132%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.32
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232860
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48448
	Voluntary context switches: 39
	Involuntary context switches: 8989
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 29, in <module>
    from keras.layers import activation
ImportError: cannot import name 'activation'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.37
	System time (seconds): 0.69
	Percent of CPU this job got: 131%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.32
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233668
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48621
	Voluntary context switches: 31
	Involuntary context switches: 28841
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 29, in <module>
    from keras.layers import activation
ImportError: cannot import name 'activation'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.40
	System time (seconds): 0.63
	Percent of CPU this job got: 130%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.33
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233264
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48476
	Voluntary context switches: 33
	Involuntary context switches: 56392
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 20, in <module>
    import keras
  File "/opt/conda/lib/python3.6/site-packages/keras/__init__.py", line 20, in <module>
    from keras import distribute
  File "/opt/conda/lib/python3.6/site-packages/keras/distribute/__init__.py", line 18, in <module>
    from keras.distribute import sidecar_evaluator
  File "/opt/conda/lib/python3.6/site-packages/keras/distribute/sidecar_evaluator.py", line 17, in <module>
    import tensorflow.compat.v2 as tf
ModuleNotFoundError: No module named 'tensorflow.compat'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.38
	System time (seconds): 0.68
	Percent of CPU this job got: 132%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 234732
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 54828
	Voluntary context switches: 30
	Involuntary context switches: 18215
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 20, in <module>
    import keras
  File "/opt/conda/lib/python3.6/site-packages/keras/__init__.py", line 20, in <module>
    from keras import distribute
  File "/opt/conda/lib/python3.6/site-packages/keras/distribute/__init__.py", line 18, in <module>
    from keras.distribute import sidecar_evaluator
  File "/opt/conda/lib/python3.6/site-packages/keras/distribute/sidecar_evaluator.py", line 17, in <module>
    import tensorflow.compat.v2 as tf
ModuleNotFoundError: No module named 'tensorflow.compat'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.36
	System time (seconds): 0.66
	Percent of CPU this job got: 133%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.27
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233936
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 54700
	Voluntary context switches: 33
	Involuntary context switches: 19963
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.50
	System time (seconds): 0.66
	Percent of CPU this job got: 131%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.42
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233592
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48645
	Voluntary context switches: 31
	Involuntary context switches: 18532
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.48
	System time (seconds): 0.66
	Percent of CPU this job got: 132%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.36
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233048
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48454
	Voluntary context switches: 40
	Involuntary context switches: 29
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.43
	System time (seconds): 0.66
	Percent of CPU this job got: 133%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.32
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 234576
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48662
	Voluntary context switches: 38
	Involuntary context switches: 25
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 2.48
	System time (seconds): 0.61
	Percent of CPU this job got: 133%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 234384
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 48528
	Voluntary context switches: 34
	Involuntary context switches: 27
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
2023-03-06 21:21:47.578702: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 21:21:47.578855: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 21:21:47.578890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using TensorFlow backend.
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.05
	System time (seconds): 0.80
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.12
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 296320
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 56739
	Voluntary context switches: 63
	Involuntary context switches: 29
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
2023-03-06 21:21:50.720971: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 21:21:50.721121: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 21:21:50.721141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using TensorFlow backend.
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.08
	System time (seconds): 0.79
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.14
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 295692
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 56865
	Voluntary context switches: 61
	Involuntary context switches: 36
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.6/imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.6/imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "compressor.py", line 20, in <module>
    import keras
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.6/imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.6/imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 1.51
	System time (seconds): 0.71
	Percent of CPU this job got: 334%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.66
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 81532
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 17893
	Voluntary context switches: 55
	Involuntary context switches: 46
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.6/imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.6/imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "decompressor.py", line 20, in <module>
    import keras
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/root/DeepZip/tf/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/usr/lib/python3.6/imp.py", line 243, in load_module
    return load_dynamic(name, filename, file)
  File "/usr/lib/python3.6/imp.py", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 1.60
	System time (seconds): 0.60
	Percent of CPU this job got: 330%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.66
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 81536
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 17934
	Voluntary context switches: 46
	Involuntary context switches: 26009
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.07
	System time (seconds): 0.77
	Percent of CPU this job got: 125%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.07
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 286300
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 57140
	Voluntary context switches: 34
	Involuntary context switches: 6320
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 21, in <module>
    from attention import *
  File "/src/workspace/src/attention.py", line 30, in <module>
    from keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.11
	System time (seconds): 0.69
	Percent of CPU this job got: 125%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.03
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 286008
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 56994
	Voluntary context switches: 35
	Involuntary context switches: 27
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 22:12:21.443463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 22:12:21.443518: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File "compressor.py", line 27, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.26
	System time (seconds): 0.69
	Percent of CPU this job got: 122%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.23
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 328692
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 75248
	Voluntary context switches: 30
	Involuntary context switches: 40671
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
2023-03-06 22:12:24.689870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-03-06 22:12:24.689930: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File "decompressor.py", line 27, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.27
	System time (seconds): 0.73
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.24
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 329340
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 75456
	Voluntary context switches: 28
	Involuntary context switches: 14622
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Traceback (most recent call last):
  File "compressor.py", line 27, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.26
	System time (seconds): 0.68
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.16
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331844
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76206
	Voluntary context switches: 32
	Involuntary context switches: 26
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 27, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.24
	System time (seconds): 0.72
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.18
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 330780
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76383
	Voluntary context switches: 28
	Involuntary context switches: 23
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Traceback (most recent call last):
  File "compressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 12, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.34
	System time (seconds): 0.74
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 343904
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76168
	Voluntary context switches: 30
	Involuntary context switches: 9131
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 36, in <module>
    import models
  File "/src/workspace/src/models.py", line 12, in <module>
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.29
	System time (seconds): 0.76
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.27
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 344504
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76425
	Voluntary context switches: 32
	Involuntary context switches: 38
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 132, in main
    tf.set_random_seed(42)
AttributeError: module 'tensorflow' has no attribute 'set_random_seed'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.28
	System time (seconds): 0.76
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.26
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331252
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76319
	Voluntary context switches: 30
	Involuntary context switches: 31
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 143, in main
    tf.set_random_seed(42)
AttributeError: module 'tensorflow' has no attribute 'set_random_seed'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.23
	System time (seconds): 0.75
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.19
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331700
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76543
	Voluntary context switches: 27
	Involuntary context switches: 41
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 23:02:46.021584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.031122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.031851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.032882: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-06 23:02:46.034010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.034765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.035476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.699331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.700067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.700752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:02:46.701389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 68, in predict_lstm
    model = getattr(models, model_name)(bs, timesteps, alphabet_size)
  File "/src/workspace/src/models.py", line 59, in Trsf
    model.add(TransformerBlock(embed_dim=32, num_heads=2, ff_dim=32))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 217, in add
    output_tensor = layer(self.outputs[0])
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 977, in __call__
    input_list)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1115, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 888, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /src/workspace/src/models.py:36 call  *
        attn_output = self.att(inputs, inputs)
    /src/workspace/src/attention.py:559 call  *
        self._build_from_signature(query=query, value=value, key=key)
    /src/workspace/src/attention.py:355 _build_from_signature  *
        self._query_dense = core.EinsumDense(

    AttributeError: module 'keras.layers.core' has no attribute 'EinsumDense'

Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 6.03
	System time (seconds): 1.58
	Percent of CPU this job got: 111%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:06.83
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1382488
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 341066
	Voluntary context switches: 199
	Involuntary context switches: 55
	Swaps: 0
	File system inputs: 0
	File system outputs: 160
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.31
	System time (seconds): 0.65
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.21
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331060
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76535
	Voluntary context switches: 28
	Involuntary context switches: 20243
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 23:03:31.660100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:31.670942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:31.671670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:31.672653: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-06 23:03:31.673770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:31.674510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:31.675215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:32.338748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:32.339526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:32.340228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:03:32.340867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 68, in predict_lstm
    model = getattr(models, model_name)(bs, timesteps, alphabet_size)
  File "/src/workspace/src/models.py", line 59, in Trsf
    model.add(TransformerBlock(embed_dim=32, num_heads=2, ff_dim=32))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 217, in add
    output_tensor = layer(self.outputs[0])
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 977, in __call__
    input_list)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1115, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 888, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /src/workspace/src/models.py:36 call  *
        attn_output = self.att(inputs, inputs)
    /src/workspace/src/attention.py:559 call  *
        self._build_from_signature(query=query, value=value, key=key)
    /src/workspace/src/attention.py:355 _build_from_signature  *
        self._query_dense = core.EinsumDense(

    AttributeError: module 'keras.layers.core' has no attribute 'EinsumDense'

Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 5.99
	System time (seconds): 1.65
	Percent of CPU this job got: 111%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:06.86
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1382908
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 341564
	Voluntary context switches: 193
	Involuntary context switches: 44
	Swaps: 0
	File system inputs: 0
	File system outputs: 160
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.28
	System time (seconds): 0.72
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.22
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331644
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76503
	Voluntary context switches: 34
	Involuntary context switches: 24
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 23:04:59.941420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:04:59.950943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:04:59.951684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:04:59.952748: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-06 23:04:59.953824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:04:59.954555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:04:59.955268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:05:00.617656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:05:00.618411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:05:00.619144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:05:00.619786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 68, in predict_lstm
    model = getattr(models, model_name)(bs, timesteps, alphabet_size)
  File "/src/workspace/src/models.py", line 59, in Trsf
    model.add(TransformerBlock(embed_dim=32, num_heads=2, ff_dim=32))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 217, in add
    output_tensor = layer(self.outputs[0])
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 977, in __call__
    input_list)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1115, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 848, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 888, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /src/workspace/src/models.py:36 call  *
        attn_output = self.att(inputs, inputs)
    /src/workspace/src/attention.py:559 call  *
        self._build_from_signature(query=query, value=value, key=key)
    /src/workspace/src/attention.py:355 _build_from_signature  *
        self._query_dense = core.EinsumDense(

    AttributeError: module 'keras.layers.core' has no attribute 'EinsumDense'

Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 6.00
	System time (seconds): 1.60
	Percent of CPU this job got: 111%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:06.83
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1382236
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 338962
	Voluntary context switches: 210
	Involuntary context switches: 38
	Swaps: 0
	File system inputs: 0
	File system outputs: 160
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.28
	System time (seconds): 0.70
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.20
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331012
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76480
	Voluntary context switches: 31
	Involuntary context switches: 31
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 23:09:12.026632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.036142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.036844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.037842: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-06 23:09:12.038748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.039491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.040170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.704513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.705256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.705933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:09:12.706541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.load_weights(args.model_weights_file)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py", line 2354, in load_weights
    with h5py.File(filepath, 'r') as f:
  File "/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py", line 408, in __init__
    swmr=swmr)
  File "/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py", line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 88, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = '../data/trained_models/xor99/Trsf.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 7.21
	System time (seconds): 1.55
	Percent of CPU this job got: 109%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:07.98
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1385504
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 342041
	Voluntary context switches: 249
	Involuntary context switches: 59
	Swaps: 0
	File system inputs: 0
	File system outputs: 288
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.22
	System time (seconds): 0.78
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.21
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331508
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76414
	Voluntary context switches: 29
	Involuntary context switches: 26
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.20
	System time (seconds): 0.78
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.21
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331316
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76522
	Voluntary context switches: 34
	Involuntary context switches: 34
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-06 23:15:30.865869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:30.875654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:30.876379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:30.877408: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-06 23:15:30.878398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:30.879152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:30.879844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:31.554750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:31.555543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:31.556219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-06 23:15:31.556856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.load_weights(args.model_weights_file)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py", line 2354, in load_weights
    with h5py.File(filepath, 'r') as f:
  File "/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py", line 408, in __init__
    swmr=swmr)
  File "/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py", line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 88, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = '../data/trained_models/xor99/Trsf.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 7.20
	System time (seconds): 1.68
	Percent of CPU this job got: 109%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:08.10
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1385316
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 340611
	Voluntary context switches: 248
	Involuntary context switches: 59
	Swaps: 0
	File system inputs: 0
	File system outputs: 288
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.22
	System time (seconds): 0.73
	Percent of CPU this job got: 120%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.28
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 330640
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76505
	Voluntary context switches: 26
	Involuntary context switches: 78316
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Starting training ...
0;0.9999439716339111
1;0.9998248815536499
2;0.9997826814651489
3;0.9997825026512146
Starting Compression ...
2023-03-07 00:20:09.022739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.036308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.037015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.038098: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 00:20:09.039319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.040039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.040725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.836883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.837628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.838299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 00:20:09.838948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.load_weights(args.model_weights_file)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py", line 2350, in load_weights
    'Unable to load weights saved in HDF5 format into a subclassed '
ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.68
	System time (seconds): 2.03
	Percent of CPU this job got: 113%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.93
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1377008
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 339833
	Voluntary context switches: 171
	Involuntary context switches: 50
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.23
	System time (seconds): 0.73
	Percent of CPU this job got: 121%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.25
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331240
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76581
	Voluntary context switches: 29
	Involuntary context switches: 53393
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:06:39.658912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:39.670393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:39.671125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:39.672155: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:06:39.673257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:39.673998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:39.674686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:40.355837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:40.356598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:40.357289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:06:40.357943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.build(input_shape=model.layers[0].input_shape)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 2117, in input_shape
    raise AttributeError('The layer has never been called '
AttributeError: The layer has never been called and thus has no defined input shape.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.68
	System time (seconds): 1.67
	Percent of CPU this job got: 114%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.57
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1378028
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 340103
	Voluntary context switches: 169
	Involuntary context switches: 64
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.24
	System time (seconds): 0.80
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.28
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331128
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76529
	Voluntary context switches: 31
	Involuntary context switches: 9246
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:07:05.020556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.030356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.031061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.032117: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:07:05.033202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.033928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.034600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.724224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.724971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.725648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:07:05.726279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.build(input_shape=model.layers[0].input_shape)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 2117, in input_shape
    raise AttributeError('The layer has never been called '
AttributeError: The layer has never been called and thus has no defined input shape.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.68
	System time (seconds): 1.68
	Percent of CPU this job got: 114%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1378980
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 340332
	Voluntary context switches: 172
	Involuntary context switches: 56
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.17
	System time (seconds): 0.80
	Percent of CPU this job got: 121%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.28
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331664
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76472
	Voluntary context switches: 26
	Involuntary context switches: 68004
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:15:53.514221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:53.524613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:53.525354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:53.526601: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:15:53.527799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:53.528530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:53.529235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:54.223744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:54.224488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:54.225196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:15:54.225814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model(np.zeros(model.layers[0].input_shape))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 2117, in input_shape
    raise AttributeError('The layer has never been called '
AttributeError: The layer has never been called and thus has no defined input shape.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.73
	System time (seconds): 1.66
	Percent of CPU this job got: 113%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.63
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1377640
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 337451
	Voluntary context switches: 172
	Involuntary context switches: 13529
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.27
	System time (seconds): 0.79
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.29
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 330744
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76497
	Voluntary context switches: 31
	Involuntary context switches: 30
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:19:00.087777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.097215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.097917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.098924: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:19:00.099900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.100650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.101317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.769540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.770275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.770944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:00.771599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model(np.zeros(1))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 383, in call
    outputs = layer(inputs, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/models.py", line 36, in call
    attn_output = self.att(inputs, inputs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/attention.py", line 595, in call
    query, key, value, attention_mask, training
  File "/src/workspace/src/attention.py", line 526, in _compute_attention
    attention_scores, attention_mask
  File "/src/workspace/src/attention.py", line 491, in _masked_softmax
    return self._softmax(attention_scores, attention_mask)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/layers/advanced_activations.py", line 347, in call
    return backend.softmax(inputs, axis=self.axis[0])
IndexError: tuple index out of range
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 7.46
	System time (seconds): 1.72
	Percent of CPU this job got: 109%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:08.40
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1659420
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 389748
	Voluntary context switches: 222
	Involuntary context switches: 84
	Swaps: 0
	File system inputs: 0
	File system outputs: 304
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.24
	System time (seconds): 0.76
	Percent of CPU this job got: 124%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.22
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331028
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76521
	Voluntary context switches: 31
	Involuntary context switches: 29
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:19:45.901733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:45.911589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:45.912298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:45.913353: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:19:45.914508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:45.915307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:45.915980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:46.583967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:46.584723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:46.585400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:19:46.586033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model(np.zeros(1))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 383, in call
    outputs = layer(inputs, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/models.py", line 36, in call
    attn_output = self.att(inputs, inputs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/attention.py", line 595, in call
    query, key, value, attention_mask, training
  File "/src/workspace/src/attention.py", line 526, in _compute_attention
    attention_scores, attention_mask
  File "/src/workspace/src/attention.py", line 491, in _masked_softmax
    return self._softmax(attention_scores, attention_mask)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/layers/advanced_activations.py", line 347, in call
    return backend.softmax(inputs, axis=self.axis[0])
IndexError: tuple index out of range
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 7.36
	System time (seconds): 1.73
	Percent of CPU this job got: 108%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:08.39
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1658936
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 388177
	Voluntary context switches: 219
	Involuntary context switches: 60307
	Swaps: 0
	File system inputs: 0
	File system outputs: 304
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.29
	System time (seconds): 0.76
	Percent of CPU this job got: 122%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331000
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76614
	Voluntary context switches: 25
	Involuntary context switches: 23582
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
  File "compressor.py", line 70
    model(np.zeros(1))
                     ^
TabError: inconsistent use of tabs and spaces in indentation
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 0.03
	System time (seconds): 0.00
	Percent of CPU this job got: 97%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.03
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8960
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 1124
	Voluntary context switches: 1
	Involuntary context switches: 0
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.27
	System time (seconds): 0.82
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331236
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76508
	Voluntary context switches: 27
	Involuntary context switches: 26
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:23:40.960218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:40.971736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:40.972443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:40.973540: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:23:40.974806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:40.975641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:40.976349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:41.655073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:41.655857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:41.656535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:23:41.657137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 204, in <module>
    main()
  File "compressor.py", line 166, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    print(model.summary())
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py", line 2521, in summary
    raise ValueError('This model has not yet been built. '
ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.83
	System time (seconds): 1.60
	Percent of CPU this job got: 113%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.64
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1378176
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 340056
	Voluntary context switches: 160
	Involuntary context switches: 56
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.26
	System time (seconds): 0.75
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.23
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331132
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76587
	Voluntary context switches: 34
	Involuntary context switches: 35
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:28:09.522039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:09.531634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:09.532346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:09.533358: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:28:09.534562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:09.535312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:09.535984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:10.209766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:10.210519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:10.211216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:28:10.211845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 204, in <module>
    main()
  File "compressor.py", line 166, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    print(model.summary())
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py", line 2521, in summary
    raise ValueError('This model has not yet been built. '
ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.86
	System time (seconds): 1.64
	Percent of CPU this job got: 113%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.72
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1381200
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 341365
	Voluntary context switches: 174
	Involuntary context switches: 55
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.42
	System time (seconds): 0.67
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.32
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 330876
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76561
	Voluntary context switches: 30
	Involuntary context switches: 35
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:29:40.341765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:40.351338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:40.352052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:40.353088: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:29:40.354273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:40.355005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:40.355745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:41.031948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:41.032689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:41.033363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:29:41.033999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.build(np.zeros(alphabet_size))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 341, in build
    self._build_graph_network_for_inferred_shape(input_shape)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 282, in _build_graph_network_for_inferred_shape
    name=self.layers[0].name + '_input')
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/input_layer.py", line 383, in Input
    input_layer = InputLayer(**input_layer_config)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/input_layer.py", line 201, in __init__
    ragged=ragged)
  File "/opt/conda/lib/python3.6/site-packages/keras/backend.py", line 1316, in placeholder
    shape=shape, dtype=dtype, name=name)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_spec.py", line 51, in __init__
    self._shape = tensor_shape.TensorShape(shape)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 765, in __init__
    self._dims = [Dimension(d) for d in dims]
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 765, in <listcomp>
    self._dims = [Dimension(d) for d in dims]
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 209, in __init__
    .format(value, type(value))), None)
  File "<string>", line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got value '0.0' with type '<class 'numpy.float64'>'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.77
	System time (seconds): 1.55
	Percent of CPU this job got: 114%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.54
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1377976
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 339093
	Voluntary context switches: 189
	Involuntary context switches: 61
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.38
	System time (seconds): 0.69
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.29
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331220
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76554
	Voluntary context switches: 26
	Involuntary context switches: 25
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:30:03.808558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:03.818171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:03.818893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:03.819926: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:30:03.820991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:03.821719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:03.822407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:04.496575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:04.497323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:04.498018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:30:04.498650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.build(np.zeros(alphabet_size))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 341, in build
    self._build_graph_network_for_inferred_shape(input_shape)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 282, in _build_graph_network_for_inferred_shape
    name=self.layers[0].name + '_input')
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/input_layer.py", line 383, in Input
    input_layer = InputLayer(**input_layer_config)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/input_layer.py", line 201, in __init__
    ragged=ragged)
  File "/opt/conda/lib/python3.6/site-packages/keras/backend.py", line 1316, in placeholder
    shape=shape, dtype=dtype, name=name)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_spec.py", line 51, in __init__
    self._shape = tensor_shape.TensorShape(shape)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 765, in __init__
    self._dims = [Dimension(d) for d in dims]
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 765, in <listcomp>
    self._dims = [Dimension(d) for d in dims]
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 209, in __init__
    .format(value, type(value))), None)
  File "<string>", line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got value '0.0' with type '<class 'numpy.float64'>'
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 4.67
	System time (seconds): 1.69
	Percent of CPU this job got: 114%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1377856
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 339719
	Voluntary context switches: 170
	Involuntary context switches: 56
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 154, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/xor99/Trsf.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.30
	System time (seconds): 0.75
	Percent of CPU this job got: 123%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.29
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 331284
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 76524
	Voluntary context switches: 26
	Involuntary context switches: 14491
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 09:43:31.590771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:31.600596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:31.601307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:31.602419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:43:31.603721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:31.604460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:31.605149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:32.322415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:32.323227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:32.323944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:43:32.324607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
2023-03-07 09:43:36.671867: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 672.60
	System time (seconds): 46.27
	Percent of CPU this job got: 103%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 11:35.33
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1780952
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 595161
	Voluntary context switches: 712319
	Involuntary context switches: 54056
	Swaps: 0
	File system inputs: 0
	File system outputs: 10776
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
2023-03-07 09:55:05.789674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:05.805021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:05.805720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:05.806733: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 09:55:05.807860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:05.808602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:05.809296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:06.645500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:06.646262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:06.646953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 09:55:06.647633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 173, in main
    series[:l] = predict_lstm(l, timesteps, batch_size, alphabet_size, args.model_name)
  File "decompressor.py", line 77, in predict_lstm
    model(np.zeros(model.layers[0].input_shape))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 2117, in input_shape
    raise AttributeError('The layer has never been called '
AttributeError: The layer has never been called and thus has no defined input shape.
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 3.84
	System time (seconds): 1.60
	Percent of CPU this job got: 116%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:04.69
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1170524
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 234604
	Voluntary context switches: 168
	Involuntary context switches: 18345
	Swaps: 0
	File system inputs: 0
	File system outputs: 8016
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
2023-03-07 10:59:31.128679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.138461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.139248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.140276: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 10:59:31.141286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.142022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.142712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.839001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.839784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.840459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 10:59:31.841087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
2023-03-07 10:59:36.192810: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 664.85
	System time (seconds): 46.33
	Percent of CPU this job got: 103%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 11:27.22
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1781112
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 596611
	Voluntary context switches: 714703
	Involuntary context switches: 55059
	Swaps: 0
	File system inputs: 0
	File system outputs: 10776
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
2023-03-07 11:10:57.246071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:57.261628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:57.262335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:57.263389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 11:10:57.264519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:57.265252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:57.265954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:58.123558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:58.124338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:58.125032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 11:10:58.125713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
Traceback (most recent call last):
  File "decompressor.py", line 185, in <module>
    main()
  File "decompressor.py", line 173, in main
    series[:l] = predict_lstm(l, timesteps, batch_size, alphabet_size, args.model_name)
  File "decompressor.py", line 77, in predict_lstm
    model(np.zeros(alphabet_size, dtype=int))
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 383, in call
    outputs = layer(inputs, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/models.py", line 36, in call
    attn_output = self.att(inputs, inputs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/attention.py", line 595, in call
    query, key, value, attention_mask, training
  File "/src/workspace/src/attention.py", line 526, in _compute_attention
    attention_scores, attention_mask
  File "/src/workspace/src/attention.py", line 491, in _masked_softmax
    return self._softmax(attention_scores, attention_mask)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/layers/advanced_activations.py", line 347, in call
    return backend.softmax(inputs, axis=self.axis[0])
IndexError: tuple index out of range
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 6.63
	System time (seconds): 1.76
	Percent of CPU this job got: 109%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:07.65
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1449928
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 283622
	Voluntary context switches: 237
	Involuntary context switches: 21978
	Swaps: 0
	File system inputs: 0
	File system outputs: 8304
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
Starting training ...
Starting Compression ...
2023-03-07 12:01:54.991616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.005780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.006500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.007918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 12:01:55.008925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.009669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.010342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.831172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.831929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.832617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:01:55.833276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
2023-03-07 12:01:57.576102: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 5119967232 exceeds 10% of free system memory.
2023-03-07 12:02:16.471585: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 76.29GiB (rounded to 81919475712)requested by op ResourceGather
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2023-03-07 12:02:16.472151: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2023-03-07 12:02:16.472171: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): 	Total Chunks: 27, Chunks in use: 27. 6.8KiB allocated for chunks. 6.8KiB in use in bin. 2.4KiB client-requested in use in bin.
2023-03-07 12:02:16.472182: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472191: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2023-03-07 12:02:16.472200: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): 	Total Chunks: 1, Chunks in use: 0. 3.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472210: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): 	Total Chunks: 3, Chunks in use: 2. 15.8KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.
2023-03-07 12:02:16.472219: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): 	Total Chunks: 5, Chunks in use: 5. 40.0KiB allocated for chunks. 40.0KiB in use in bin. 40.0KiB client-requested in use in bin.
2023-03-07 12:02:16.472228: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472236: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472244: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472252: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472260: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472268: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472276: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472284: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472292: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472300: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472308: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472316: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472324: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472331: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-03-07 12:02:16.472341: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): 	Total Chunks: 2, Chunks in use: 1. 7.22GiB allocated for chunks. 4.77GiB in use in bin. 4.77GiB client-requested in use in bin.
2023-03-07 12:02:16.472352: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 76.29GiB was 256.00MiB, Chunk State: 
2023-03-07 12:02:16.472366: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 2.45GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 8.0KiB | Requested Size: 8.0KiB | in_use: 1 | bin_num: -1
2023-03-07 12:02:16.472373: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 7751598080
2023-03-07 12:02:16.472384: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8e84000000 of size 256 next 1
2023-03-07 12:02:16.472391: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8e84000100 of size 1280 next 2
2023-03-07 12:02:16.472397: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8e84000600 of size 5119967232 next 3
2023-03-07 12:02:16.472404: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8600 of size 256 next 4
2023-03-07 12:02:16.472538: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8700 of size 256 next 5
2023-03-07 12:02:16.472549: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8800 of size 256 next 6
2023-03-07 12:02:16.472556: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8900 of size 256 next 7
2023-03-07 12:02:16.472562: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8a00 of size 256 next 8
2023-03-07 12:02:16.472568: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8b00 of size 256 next 9
2023-03-07 12:02:16.472575: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8c00 of size 256 next 10
2023-03-07 12:02:16.472581: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8d00 of size 256 next 11
2023-03-07 12:02:16.472588: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c8e00 of size 4096 next 30
2023-03-07 12:02:16.472595: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52c9e00 of size 4096 next 32
2023-03-07 12:02:16.472601: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f8fb52cae00 of size 7936 next 12
2023-03-07 12:02:16.472607: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52ccd00 of size 8192 next 13
2023-03-07 12:02:16.472613: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52ced00 of size 256 next 14
2023-03-07 12:02:16.472619: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cee00 of size 256 next 17
2023-03-07 12:02:16.472634: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cef00 of size 256 next 19
2023-03-07 12:02:16.472641: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf000 of size 256 next 20
2023-03-07 12:02:16.472647: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf100 of size 256 next 23
2023-03-07 12:02:16.472653: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf200 of size 256 next 24
2023-03-07 12:02:16.472659: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf300 of size 256 next 25
2023-03-07 12:02:16.472665: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf400 of size 256 next 26
2023-03-07 12:02:16.472671: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf500 of size 256 next 27
2023-03-07 12:02:16.472677: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf600 of size 256 next 28
2023-03-07 12:02:16.472683: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf700 of size 256 next 29
2023-03-07 12:02:16.472689: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf800 of size 256 next 31
2023-03-07 12:02:16.472695: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cf900 of size 256 next 33
2023-03-07 12:02:16.472701: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cfa00 of size 256 next 34
2023-03-07 12:02:16.472707: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cfb00 of size 256 next 35
2023-03-07 12:02:16.472713: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cfc00 of size 256 next 36
2023-03-07 12:02:16.472720: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cfd00 of size 256 next 37
2023-03-07 12:02:16.472727: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52cfe00 of size 256 next 38
2023-03-07 12:02:16.472733: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f8fb52cff00 of size 3840 next 15
2023-03-07 12:02:16.472739: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52d0e00 of size 8192 next 16
2023-03-07 12:02:16.472745: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52d2e00 of size 8192 next 18
2023-03-07 12:02:16.472754: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52d4e00 of size 8192 next 22
2023-03-07 12:02:16.472760: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f8fb52d6e00 of size 8192 next 21
2023-03-07 12:02:16.472767: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f8fb52d8e00 of size 2631561728 next 18446744073709551615
2023-03-07 12:02:16.472773: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: 
2023-03-07 12:02:16.472787: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 27 Chunks of size 256 totalling 6.8KiB
2023-03-07 12:02:16.472795: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB
2023-03-07 12:02:16.472803: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 4096 totalling 8.0KiB
2023-03-07 12:02:16.472811: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 8192 totalling 40.0KiB
2023-03-07 12:02:16.472819: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 5119967232 totalling 4.77GiB
2023-03-07 12:02:16.472826: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 4.77GiB
2023-03-07 12:02:16.472834: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 7751598080 memory_limit_: 7751598080 available bytes: 0 curr_region_allocation_bytes_: 15503196160
2023-03-07 12:02:16.472847: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: 
Limit:                      7751598080
InUse:                      5120024576
MaxInUse:                   5120028928
NumAllocs:                          64
MaxAllocSize:               5119967232
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-03-07 12:02:16.472859: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *******************************************************************_________________________________
2023-03-07 12:02:16.472940: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at resource_variable_ops.cc:698 : Resource exhausted: OOM when allocating tensor with shape[9999936,64,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "compressor.py", line 203, in <module>
    main()
  File "compressor.py", line 165, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model(X)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/sequential.py", line 369, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/functional.py", line 415, in call
    inputs, training=training, mask=mask)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/functional.py", line 550, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/src/workspace/src/models.py", line 53, in call
    x = self.token_emb(x)
  File "/opt/conda/lib/python3.6/site-packages/keras/engine/base_layer.py", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/keras/layers/embeddings.py", line 191, in call
    out = tf.nn.embedding_lookup(self.embeddings, inputs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py", line 206, in wrapper
    return target(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py", line 395, in embedding_lookup_v2
    return embedding_lookup(params, ids, "div", name, max_norm=max_norm)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py", line 206, in wrapper
    return target(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py", line 329, in embedding_lookup
    transform_fn=None)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py", line 138, in _embedding_lookup_and_transform
    array_ops.gather(params[0], ids, name=name), ids, max_norm)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 549, in new_func
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py", line 206, in wrapper
    return target(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py", line 5056, in gather
    return params.sparse_read(indices, name=name)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 714, in sparse_read
    self.handle, indices, dtype=self._dtype, name=name)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py", line 549, in resource_gather
    _ops.raise_from_not_ok_status(e, name)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[9999936,64,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/xor99.npy -data_params ../data/processed_files/xor99.param.json -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -output ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 10.02
	System time (seconds): 7.16
	Percent of CPU this job got: 65%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:26.40
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 11376964
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 1592374
	Voluntary context switches: 463
	Involuntary context switches: 62
	Swaps: 0
	File system inputs: 0
	File system outputs: 304
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
2023-03-07 12:02:19.979195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:19.989167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:19.989899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:19.990925: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-07 12:02:19.991847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:19.992566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:19.993235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:20.676281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:20.677020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:20.677703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-03-07 12:02:20.678334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:00:06.0, compute capability: 6.1
2023-03-07 12:02:23.181965: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
{'0': 97, '1': 98}
[0 1 1 1 0 0 0 0 1 1]
	Command being timed: "python decompressor.py -output ../data/compressed/xor99/Trsf.reconstructed.txt -model ../data/trained_models/xor99/Trsf.hdf5 -model_name Trsf -input_file_prefix ../data/compressed/xor99/Trsf.compressed -batch_size 1000"
	User time (seconds): 765.10
	System time (seconds): 46.35
	Percent of CPU this job got: 103%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 13:06.80
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1626792
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 510182
	Voluntary context switches: 704005
	Involuntary context switches: 71461
	Swaps: 0
	File system inputs: 0
	File system outputs: 27856
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
